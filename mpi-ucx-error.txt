$ mpirun -np 2 -mca pml ucx ./build/bin/gtable_test data/mydata/cities.csv 
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              r-005
  Local adapter:           mlx4_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There is at least non-excluded one OpenFabrics device found,
but there are no active ports detected (or Open MPI was unable to use
them).  This is most certainly not what you wanted.  Check your
cables, subnet manager configuration, etc.  The openib BTL will be
ignored for this job.

  Local host: r-005
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A requested component was not found, or was unable to be opened.  This
means that this component is either not installed or is unable to be
used on your system (e.g., sometimes this means that shared libraries
that the component requires are unable to be found/loaded).  Note that
Open MPI stopped checking at the first component that it did not find.

Host:      r-005.juliet.futuresystems.org
Framework: pml
Component: ucx
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  mca_pml_base_open() failed
  --> Returned "Not found" (-13) instead of "Success" (0)
--------------------------------------------------------------------------
[r-005:27201] *** An error occurred in MPI_Init
[r-005:27201] *** reported by process [2262499329,1]
[r-005:27201] *** on a NULL communicator
[r-005:27201] *** Unknown error
[r-005:27201] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[r-005:27201] ***    and potentially your MPI job)
[r-005.juliet.futuresystems.org:27191] PMIX ERROR: UNREACHABLE in file ../../../../../../../opal/mca/pmix/pmix3x/pmix/src/server/pmix_server.c at line 2147
[r-005.juliet.futuresystems.org:27191] 1 more process has sent help message help-mpi-btl-openib.txt / ib port not selected
[r-005.juliet.futuresystems.org:27191] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[r-005.juliet.futuresystems.org:27191] 1 more process has sent help message help-mpi-btl-openib.txt / no active ports found
[r-005.juliet.futuresystems.org:27191] 1 more process has sent help message help-mca-base.txt / find-available:not-valid
[r-005.juliet.futuresystems.org:27191] 1 more process has sent help message help-mpi-runtime.txt / mpi_init:startup:internal-failure
[r-005.juliet.futuresystems.org:27191] 1 more process has sent help message help-mpi-errors.txt / mpi_errors_are_fatal unknown handle

